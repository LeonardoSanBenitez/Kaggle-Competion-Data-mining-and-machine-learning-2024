from typing import List, Literal
import logging
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.decomposition import PCA


class PandasBaseEstimator(BaseEstimator):
    '''
    Base class for custom transformers that preserve pandas DataFrame.
    Useful if you want to interpret the model afterwards, so the name of the variables is preserved.
    Also eases debugging, at the expense of performance and writting more code.
    '''
    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'PandasBaseEstimator':
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        return X


class PandasStandardScaler(PandasBaseEstimator, TransformerMixin):
    def __init__(self):
        self.scaler = StandardScaler()

    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'PandasStandardScaler':
        assert isinstance(X, pd.DataFrame), "X must be a pandas DataFrame."
        if X.isnull().any().any():
            raise ValueError("X contains missing values, which are not supported.")
        self.scaler.fit(X, y)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        assert isinstance(X, pd.DataFrame), "X must be a pandas DataFrame."
        if X.isnull().any().any():
            raise ValueError("X contains missing values, which are not supported.")
        scaled_array = self.scaler.transform(X)
        return pd.DataFrame(scaled_array, index=X.index, columns=X.columns)


class PandasPCA(PandasBaseEstimator, TransformerMixin):
    n_components: float
    pca: PCA
    columns: List[str] = []

    def __init__(self, n_components: float):
        assert 0 < n_components <= 1, "n_components must be a float between 0 and 1."
        self.n_components = n_components

    def fit(self, X: pd.DataFrame, y: pd.Series) -> 'PandasPCA':
        if self.n_components is not None:
            n_components = int(X.shape[1] * self.n_components)
            logging.info(f"PCA: Keeping {n_components} components out of {X.shape[1]}")
        else:
            n_components = None
        self.pca = PCA(n_components=n_components)
        self.pca.fit(X, y)
        self.columns = [f'PC{i+1}' for i in range(self.pca.n_components_)]
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        transformed = self.pca.transform(X)
        return pd.DataFrame(transformed, index=X.index, columns=self.columns)


class RemoveUncorrelated(PandasBaseEstimator, TransformerMixin):
    threshold: float
    dropped_columns: list[str]

    def __init__(self, threshold: float = 0.1):
        """
        Base code generated by ChatGPT. Conversation history: https://chatgpt.com/share/673c8c6e-6f90-800f-bdaa-4767b688f923

        Args:
            threshold (float): Minimim correlation. The correlation threshold below which columns will be dropped.
        """
        assert 0 <= threshold <= 1, "Threshold must be a float between 0 and 1."
        self.threshold = threshold
        self.dropped_columns = []

    def fit(self, X: pd.DataFrame, y: pd.Series, method: Literal['pearson', 'kendall', 'spearman'] = 'spearman') -> 'RemoveUncorrelated':
        """
        Fit the transformer by calculating correlations between columns of X and the target y.

        Args:
            X (pd.DataFrame): The input features.
            y (pd.Series): The target variable.

        Returns:
            MyCustomTransformer: Fitted transformer.
        """
        X = X.copy()
        assert isinstance(X, pd.DataFrame), "X must be a pandas DataFrame."
        assert isinstance(y, pd.Series), "y must be a pandas Series."
        assert len(X) == len(y), "X and y must have the same length."

        # Identify constant columns with standard deviation 0
        constant_columns = X.columns[X.std() <= 0.00000001].tolist()
        if constant_columns:
            logging.debug(f"Columns with zero standard deviation: {constant_columns}")
            self.dropped_columns.extend(constant_columns)
            X = X.drop(columns=constant_columns)

        # Remove rows with null values
        clean_data = pd.concat([X, y], axis=1).dropna()
        removed_fraction = 1 - len(clean_data) / len(X)
        if removed_fraction > 0.9:
            logging.warning(f"{removed_fraction:.2%} of the dataset rows were removed due to null values ({len(clean_data)} rows remaining out of {len(X)})")
        X = clean_data.iloc[:, :-1]
        y = clean_data.iloc[:, -1]

        # Compute correlations
        correlations = X.corrwith(y, method=method)
        low_corr_columns = correlations[correlations.abs() < self.threshold].index.tolist()
        self.dropped_columns.extend(low_corr_columns)
        removed_fraction_cols = len(self.dropped_columns) / X.shape[1]
        if removed_fraction_cols > 0.9:
            logging.warning(f"{removed_fraction_cols:.2%} of the columns were removed due to low correlation ({X.shape[1] - len(self.dropped_columns)} columns remaining out of {X.shape[1]})")

        # Log the information
        #logging.info(f"Dropping columns due to low correlation: {low_corr_columns}")
        logging.info(f"Dropping {len(low_corr_columns)} columns due to low correlation")
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Transform the input data by removing columns with low correlation.

        Args:
            X (pd.DataFrame): The input features.

        Returns:
            pd.DataFrame: Transformed DataFrame with low-correlation columns removed.
        """
        assert isinstance(X, pd.DataFrame), f"X must be a pandas DataFrame, got {type(X)}"
        return X.drop(columns=self.dropped_columns, errors='ignore')
