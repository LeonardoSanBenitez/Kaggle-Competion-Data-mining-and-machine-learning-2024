from typing import Dict, Any, List
import hashlib
import logging
import json
import time
import numpy as np
import pandas as pd

from sklearn.metrics import balanced_accuracy_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import balanced_accuracy_score
from sklearn.pipeline import Pipeline
from hyperopt import STATUS_OK, STATUS_FAIL
import mlflow
import shap

from libs import PandasStandardScaler, PandasPCA, RemoveUncorrelated


def calculate_params_hash(params: Dict[str, Any]) -> str:
    stringified = json.dumps({k: str(params[k]) for k in params}, sort_keys=True)
    return hashlib.md5(stringified.encode()).hexdigest()


def train_model(params: Dict[str, Any], X: pd.DataFrame, y: pd.Series, analyze: bool = False, analyze_n: int = 1000, analyze_features: List[int] = []) -> Any:
    '''
    @return fitted sklearn-like model (have a method `predict(X)`, etc)
    '''
    params = params.copy()
    ModelClass = params.pop('model')
    assert params.pop('framework') in ['scikit-learn~=1.4', 'xgboost~=2.1']

    pipeline_steps = []
    if 'preprocessing_remove_uncorrelated' in params:
        pipeline_steps.append(('remove_uncorrelated', RemoveUncorrelated(threshold=params.pop('preprocessing_remove_uncorrelated'))))
    if 'preprocessing_scale' in params and params['preprocessing_scale']:
        pipeline_steps.append(('standardize', PandasStandardScaler()))
        params.pop('preprocessing_scale')
    if 'preprocessing_pca' in params:
        pipeline_steps.append(('pca', PandasPCA(n_components=params.pop('preprocessing_pca'))))
    pipeline_steps.append(('model', ModelClass(**params)))
    pipeline = Pipeline(pipeline_steps)
    pipeline.fit(X, y)

    if analyze:
        try:
            shap.initjs()
            # TODO: choose the explained based on the model type
            explainer = shap.TreeExplainer(pipeline.named_steps['model'])
            explanations = explainer(X.sample(n=1000, random_state=1))

            # Waterfall plot of shap values for one single prediction
            # f(x): predicted target for that row
            # E[f(X)]: average target
            shap.plots.waterfall(explanations[0])

            # Force plot of shap values for one single prediction
            # It's like a stacked waterfall
            shap.plots.force(explanations[0])
            shap.plots.force(explanations, ordering_keys=explanations.sum(1))

            # Headmap (rows are instances, columns are features)
            shap.plots.heatmap(explanations, instance_order=explanations.sum(1))

            # Summary of feature importance
            shap.plots.bar(explanations)

            # Beeswarm plot (like a violin plot, points are shap values)
            shap.plots.beeswarm(explanations)

            # Dependence plot
            # scatter of shap (y) vs one feature
            if len(analyze_features) == 0:
                analyze_features = [explanations.abs.mean(0).values.argmax()]  # Most important feature
            for feature in analyze_features:
                shap.plots.scatter(explanations[:, feature])

            # TODO:
            # Return the graphs so that they can be logged by ML flow
            # Or save them: plt.savefig("shap_waterfall.png")
        except Exception as e:
            logging.error(f'Unexpected error at SHAP: {e}')
    return pipeline


def objective(params: dict, X: pd.DataFrame, y: pd.Series) -> dict:
    # The k-fold logic was generated by chatGPT: https://chatgpt.com/share/6729189d-8bec-800f-9ab3-d4ec6d9617f6
    try:
        assert isinstance(params, dict)
        assert isinstance(X, pd.DataFrame)
        assert isinstance(y, pd.Series)

        kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)
        with mlflow.start_run(nested=True):
            mlflow.log_params(params)
            mlflow.log_param('hyperparameters_hash', calculate_params_hash(params))
            logging.info("-"*50)
            logging.info(f"Trying out params {calculate_params_hash(params)}   -> {params}")
            scores = []
            t0 = time.time()
            for train_ix, test_ix in kfold.split(X, y):
                train_X, test_X = pd.DataFrame(X.iloc[train_ix]), pd.DataFrame(X.iloc[test_ix])
                train_y, test_y = y.iloc[train_ix], y.iloc[test_ix]
                assert isinstance(train_X, pd.DataFrame)
                assert isinstance(test_X, pd.DataFrame)
                assert isinstance(train_y, pd.Series)
                assert isinstance(test_y, pd.Series)
                model = train_model(params, train_X, train_y)
                y_pred = model.predict(test_X)
                score = balanced_accuracy_score(test_y, y_pred)
                scores.append(score)
                logging.info(f'Trained model on {train_X.shape[0]} rows, got score {score}')
            metric = np.mean(scores)
            logging.info(f"Final metric: {metric}")
            mlflow.log_metric("balanced_accuracy_score_mean", metric)
            mlflow.log_metric("time_train", (time.time() - t0)/3)
        return {'loss': -metric, 'status': STATUS_OK}
    except Exception as e:
        logging.exception(e)
        return {'loss': 666, 'status': STATUS_FAIL}
