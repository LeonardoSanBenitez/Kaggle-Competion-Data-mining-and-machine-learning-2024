{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb9275-2cfc-4aec-89b0-bceae9f17487",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /src/code\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "import subprocess\n",
    "import hashlib\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, STATUS_FAIL\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt import space_eval\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import mlflow\n",
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "from libs import (\n",
    "    PandasStandardScaler,\n",
    "    PandasPCA,\n",
    "    RemoveUncorrelated,\n",
    "    calculate_params_hash,\n",
    "    train_model,\n",
    "    objective\n",
    ")\n",
    "\n",
    "random.seed(42)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "pd.set_option('display.max_columns', None)\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "target_col: str = 'failure_prone'\n",
    "submission_id: str = 'U8AXLZ'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f22973-be4f-4177-aeaa-0094c71e34fc",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3a796-b793-49b7-bb28-850fbde84e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../assets/X_train.csv', index_col='id')\n",
    "y = pd.read_csv('../assets/y_train.csv', index_col='id')[target_col].astype(int)\n",
    "\n",
    "print(X.shape)\n",
    "print(X.info())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad593941-629e-4954-a7f2-ff3c797c97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.info())\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aee5a4-0e14-4a76-8dce-9391507a30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "profile = ProfileReport(\n",
    "    df = pd.concat(\n",
    "        [X, y],\n",
    "        axis=1,\n",
    "    ).sample(n=10000, random_state=1).rename(columns={'failure_prone': \"Target\"}),  # Reduce size\n",
    "    title=\"Profiling Report\",\n",
    "    # minimal=True,  # Reduces the size by showing less graphs; Not needed in our case\n",
    ")\n",
    "profile.config.interactions.targets = [\"Target\"]  # Reduce size by disabling most interactions\n",
    "profile.to_file(\"../assets/profiling_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436648b-8900-4111-9952-064e6ce5b0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "profile = ProfileReport(\n",
    "    df = pd.concat(\n",
    "        [X, y],\n",
    "        axis=1,\n",
    "    ).sample(n=30000, random_state=1).rename(columns={'failure_prone': \"Target\"}),\n",
    "    title=\"Profiling Report - Full\",\n",
    "    explorative=True,\n",
    ")\n",
    "\n",
    "profile.to_file(\"../assets/profiling_report_full.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143497dc-614b-4f62-bf71-a796cc4196e8",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "tricks for conditional spaces: https://stackoverflow.com/questions/43859465/problems-setting-up-conditional-search-space-in-hyperopt\n",
    "\n",
    "mlflow code copied from: https://github.com/LeonardoSanBenitez/tutorial-mlflow/blob/main/code/2.0%20-%20AutoML%20with%20hyperopt.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34fc96-dc6e-4457-8f50-73e0c3880d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "space = hp.choice('classifier', [\n",
    "    ############\n",
    "    # Run 3\n",
    "    # 0.8876\n",
    "    {\n",
    "        'model': XGBClassifier,\n",
    "        'framework': 'xgboost~=2.1',\n",
    "        'learning_rate': hp.uniform('xgb_learning_rate', 0.4, 0.5),\n",
    "        'n_estimators': scope.int(hp.quniform('xgb_n_estimators', 1400, 1600, 25)),\n",
    "        'max_depth': scope.int(hp.quniform('xgb_max_depth', 10, 20, 2)),\n",
    "        'random_state': 0,\n",
    "        'preprocessing_remove_uncorrelated': hp.uniform('gb_preprocessing_remove_uncorrelated', 0.1, 0.15),\n",
    "        'preprocessing_scale': True,\n",
    "        'preprocessing_pca': hp.uniform('gb_preprocessing_pca', 0.8, 0.9),\n",
    "    }\n",
    "\n",
    "    ################\n",
    "    # Run 2\n",
    "    # 0.8828\n",
    "    #{\n",
    "    #    'model': XGBClassifier,\n",
    "    #    'framework': 'xgboost~=2.1',\n",
    "    #    'learning_rate': 0.4928067035150168,\n",
    "    #    'n_estimators': 200,\n",
    "    #    'max_depth': 10,        \n",
    "    #    'random_state': 0,\n",
    "    #    'preprocessing_remove_uncorrelated': hp.uniform('gb_preprocessing_remove_uncorrelated', 0.0, 0.2),\n",
    "    #    'preprocessing_scale': True,\n",
    "    #    'preprocessing_pca': hp.uniform('gb_preprocessing_pca', 0.1, 1.0),\n",
    "    #}\n",
    "\n",
    "    ################\n",
    "    # Run 1\n",
    "    # 0.8813\n",
    "    #{\n",
    "    #    'model': RandomForestClassifier,\n",
    "    #    'framework': 'scikit-learn~=1.4',\n",
    "    #    'n_estimators': scope.int(hp.quniform('rf_n_estimators', 50, 200, 50)),  # discrite values from 50 to 500, every 50\n",
    "    #    'max_depth': scope.int(hp.quniform('rf_max_depth', 10, 100, 10)),\n",
    "    #    'random_state': 0,\n",
    "    #},\n",
    "    #{\n",
    "    #    'model': GradientBoostingClassifier,\n",
    "    #    'framework': 'scikit-learn~=1.4',\n",
    "    #    'learning_rate': hp.uniform('gb_learning_rate', 0.1, 0.5),  # continuous range from 0.01 to 0.5\n",
    "    #    'n_estimators': scope.int(hp.quniform('gb_n_estimators', 100, 200, 50)),\n",
    "    #    'max_depth': 10, #scope.int(hp.quniform('gb_max_depth', 3, 30, 2)),\n",
    "    #    'random_state': 0,\n",
    "    #},\n",
    "    #{\n",
    "    #    'model': KNeighborsClassifier,\n",
    "    #    'framework': 'scikit-learn~=1.4',\n",
    "    #    'n_neighbors': scope.int(hp.quniform('knn_n_neighbors', 5, 50, 2)),\n",
    "    #},\n",
    "    #{\n",
    "    #    'model': SVC,\n",
    "    #    'framework': 'scikit-learn~=1.4',\n",
    "    #    'C': hp.uniform('svm_C', 0.1, 10),  # continuous range from 0.1 to 10\n",
    "    #    'kernel': hp.choice('svm_kernel', ['poly', 'rbf']),\n",
    "    #    #'degree': scope.int(hp.quniform('svm_degree', 2, 5, 1)),  # only used for 'poly' kernel\n",
    "    #    #'gamma': hp.choice('svm_gamma', ['scale', 'auto']),\n",
    "    #    'random_state': 0,\n",
    "    #},\n",
    "    #{\n",
    "    #    'model': MLPClassifier,\n",
    "    #    'framework': 'scikit-learn~=1.4',\n",
    "    #    'hidden_layer_sizes': scope.int(hp.quniform('mlp_hidden_layer_sizes', 50, 200, 50)),\n",
    "    #    'activation': hp.choice('mlp_activation', ['tanh', 'relu']),\n",
    "    #    #'solver': hp.choice('mlp_solver', ['sgd', 'adam']),\n",
    "    #    #'alpha': hp.uniform('mlp_alpha', 0.0001, 0.01),\n",
    "    #    #'learning_rate': hp.choice('mlp_learning_rate', ['invscaling', 'adaptive']),\n",
    "    #    'random_state': 0,\n",
    "    #},\n",
    "    #{\n",
    "    #    'model': XGBClassifier,\n",
    "    #    'framework': 'xgboost~=2.1',\n",
    "    #    'learning_rate': hp.uniform('xgb_learning_rate', 0.3, 0.5),\n",
    "    #    'n_estimators': scope.int(hp.quniform('xgb_n_estimators', 100, 200, 50)),\n",
    "    #    'max_depth': 10, #scope.int(hp.quniform('xgb_max_depth', 10, 30, 2)),\n",
    "    #    'random_state': 0,\n",
    "    #}\n",
    "])\n",
    "\n",
    "mlflow.set_tracking_uri('http://mlflow:5000')\n",
    "mlflow.set_experiment(\"round_3\")\n",
    "with mlflow.start_run(description='Parent run for hyperopt', nested=True):\n",
    "    best_hyperparameters: Dict[str, Any] = space_eval(space, fmin(\n",
    "        fn=lambda params: objective(params, X, y),  # Objective function to minimize\n",
    "        space=space,                                # Hyperparameter space\n",
    "        algo=tpe.suggest,                           # Optimization algorithm (Tree of Parzen Estimators)\n",
    "        max_evals=700,                              # Number of evaluations\n",
    "        trials=Trials()                             # Store trial history for later inspection\n",
    "    ))\n",
    "\n",
    "    assert type(best_hyperparameters) == dict\n",
    "    print(\"Best hyperparameters found:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77e207-6154-464f-8b1c-d597c77637b3",
   "metadata": {},
   "source": [
    "# Train final model\n",
    "expects a dictionary `best_hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6b7a3-9cdb-4026-b866-4196aae4c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TODO: register at mlflow\n",
    "best_hash = calculate_params_hash(best_hyperparameters)\n",
    "print(f'Search experiment by param hash in the UI: http://localhost:5001/#/experiments/127862172636814249?searchFilter=params.hyperparameters_hash+%3D\"{best_hash}\"')\n",
    "\n",
    "model = train_model(best_hyperparameters, X, y, analyze=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.named_steps['model'].feature_importances_\n",
    "columns = model.named_steps['pca'].columns\n",
    "\n",
    "# Convert importances to percentage\n",
    "importances_percentage = importances / importances.sum() * 100\n",
    "\n",
    "# Create a dataframe for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': columns,\n",
    "    'Importance (%)': importances_percentage\n",
    "}).sort_values(by='Importance (%)', ascending=False)\n",
    "\n",
    "# Display the dataframe\n",
    "feature_importances_df.head(10)  # Display top 10 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3bcd23-ad6d-482f-9845-2abf82813db5",
   "metadata": {},
   "source": [
    "# Submit best model\n",
    "Expects a variable `model`, with a method `predict` that follows a sklean-like interface, and a dictionary `hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce4aee6-163a-4f0a-85fb-ebf2016550cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model is not None\n",
    "assert hasattr(model, 'predict')\n",
    "\n",
    "X_test = pd.read_csv('../assets/X_test.csv', index_col='id')\n",
    "assert X_test.shape[1] == X.shape[1]\n",
    "assert all(X_test.columns == X.columns)\n",
    "\n",
    "y_test_hat = model.predict(X_test)\n",
    "assert len(y_test_hat.shape) == 1\n",
    "assert y_test_hat.shape[0] == X_test.shape[0]\n",
    "\n",
    "assert y_test_hat.min() == 0\n",
    "assert y_test_hat.max() == 1\n",
    "assert y_test_hat.dtype == np.dtype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e8993-2506-4bd7-8beb-29bd5a457624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission params\n",
    "competition = 'ppcu-data-mining-and-machine-learning-2024'\n",
    "file_name = f'../assets/y_test_submission_{submission_id}.csv'\n",
    "message = f\"test trained submission\"\n",
    "message += f\"; commit_hash: {subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode('ascii').strip()}\"\n",
    "message += f\"; hyperparameters_hash: {calculate_params_hash(best_hyperparameters)}\"\n",
    "\n",
    "# Run inference\n",
    "if os.path.exists(file_name):\n",
    "    os.remove(file_name)\n",
    "pd.DataFrame(y_test_hat, index=X_test.index, columns=[\"failure_prone\"]).to_csv(file_name, index=True)\n",
    "assert os.path.exists(file_name)\n",
    "\n",
    "# Submit the file to the competition\n",
    "api.competition_submit(file_name, message, competition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
